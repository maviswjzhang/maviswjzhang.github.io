---
layout:     post
title:      HRNet v2阅读笔记及代码理解
subtitle:   High-Resolution Representations for Labeling Pixels and Regions
date:       2019-07-01
author:     Nick
header-img: img/博客背景.jpg
catalog: true
tags:
    - 论文阅读摘要
---

## 摘要
高分辨率表示学习在许多视觉问题中起着重要作用，例如姿势估计和语义分割。最近开发用于人体姿态估计的高分辨率网络（HRNet）[91]通过并行连接高到低分辨率的卷积在整个过程中保持高分辨率表示，并通过反复进行平行的卷积融合产生高分辨率表示。

在本文中，我们通过引入简单而有效的修改并将其应用于广泛的视觉任务，对高分辨率表示进行了进一步的研究。我们通过聚合来自所有并行卷积的（上采样）表示来增强高分辨率表示，而不是如[91]中所做的那样仅仅来自高分辨率卷积的表示。我们卓越的实验结果可以证明这种简单的修改可以产生更强的表现形式。我们展示了n Cityscapes, LIP, and PASCAL Context, and facial landmark detection on AFLW, COFW, 300W, and WFLW的表面地标检测的最佳结果。此外，我们从高分辨率表示出发构建多级表示，并将其应用于faster RCNN目标检测框架和扩展框架。所提出的方法对比于现有的单模型网络在COCO对象检测上实现了优异的结果。代码和模型已在https://github.com/HRNet上公开。

## 1. 简介

深度学习的表现形式已被证明是强大的，并在许多视觉任务中取得了最先进的成果。主要有两种表示形式：一是用于图像分类的低分辨率表示；二是对包括语义分割、物体检测、人体姿势估计等在内的许多其他视觉问题至关重要的高分辨率表示。后者是本文的关注点，它仍未得到最合适的解决并且吸引了大量关注。

计算高分辨率表示有两条主线。一种是从网络（例如，ResNet）输出的低分辨率表示中恢复高分辨率表示，并且可选地恢复中间中等分辨率表示，例如Hourglass [72]，SegNet [2]，DeconvNet [74]，U-Net [83]，和encoderdecoder [77]。 另一个是通过高分辨率卷积保持高分辨率表示，并用并行的低分辨率卷积加强表示[91,30,132,86]。 此外，空洞卷积用于替换分类网络中的一些步长卷积和相关的常规卷积，以计算中等分辨率表示[13,126]。

我们通过探索所有高分辨率和低分辨率并行卷积的表示而不仅仅是原始HRNet [91]中的高分辨率表示对原始网络进行了简单的修改。此修改增加了较小的开销，但是有着更强的高分辨率表示。我们把修改过后的网络称为HRNetV2。 我们凭经验证明了其相对于原始HRNet的优越性。

我们通过从输出的高分辨率表示来估计分割图/表面地标热图来将我们提出的网络应用于语义分割/面部标志检测。 在语义分割中，我们所提出的方法在具有相似模型大小和较低计算复杂度的PASCAL Context, Cityscapes, and LIP上实现了最先进的结果。 在面部标志检测中，我们的方法在AFLW，COFW，300W和WFLW这四个标准数据集上实现了总体最佳结果。

此外，我们从高分辨率表示构建了多级表示，并将其应用于更快的R-CNN对象检测框架及其扩展框架—Mask R-CNN [38]和Cascade R-CNN [9]。结果表明，我们的方法获得了很好的检测性能，特别是对小物体有着显著改进。通过单一规模的训练和测试，所提出的方法比现有的单模型方法实现了更好的COCO对象检测结果。

![HRNer V2网络结构](/img/2019-07-01-1.png)

## 2. 相关工作

强大的高分辨率表示在像素和区域标记问题中起重要作用，例如，语义分割，人体姿势估计，面部标志检测和对象检测。我们从低分辨率表示的学习，高分辨率表示的恢复到高分辨率表示的保持回顾了主要在语义分割，面部地标检测[92,50,69,104,123,94,119]和目标检测区域1中表示学习技术的发展。

**学习低分辨率表示：**全卷积网络（FCN）实现了[67,87]通过去除分类网络中的完全连接的层来计算低分辨率表示，并通过其粗分割的置信度图进行估计。它通过连接从中间低级别的中等分辨率表示或者迭代过程[50]中估计得到的精细分割得分图[67]来改进估计的分割图。类似的技术也已应用于边缘检测，例如整体边缘检测[106]。

完全卷积网络通过将一些（通常是两个）步幅卷积和相关卷积用扩张卷积替换为扩张版本来进行发展改进，从而得到中等分辨率表示[126,13,115,12,57]。这种表示通过从多个尺度分割对象的特征金字塔来进一步扩展到多尺度的上下文表示[126，13,15]。

**恢复高分辨率表示：**采用上采样子网，如解码器，逐步恢复从下采样过程输出的低分辨率表示逐步恢复到高分辨率表示。上采样子网可以是下采样子网的对称版本，在一些镜像层上跳过连接以转换池化索引，例如SegNet [2]和DeconvNet [74]，或复制特征映射，例如U-Net [83]和Hourglass [72,111,7,22,6]，编码器-解码器[77]，FPN [62]等。全分辨率残差网络[78]引入了额外的可以在全图像分辨率上携带信息的全分辨率流以替换跳过连接，并且下采样和上采样子网中的每个单元从全分辨率流中接收信息，同时也向全分辨率流发送信息。

非对称上采样过程也得到了广泛的研究.RefineNet [60]改进了上采样表示和从下采样过程复制的相同分辨率的表示的组合。其他工作包括：轻型上采样过程[5]; 轻型下采样和重上采样过程[97]，重组器网络[40]; 利用更多或更复杂的卷积单元[76,125,42]改进跳过连接，以及将信息从低分辨率跳过连接发送到高分辨率跳过连接[133]或在它们之间交换信息[36]; 研究上采样过程的细节[100]; 结合多尺度金字塔表示[16,105]; 堆叠多个DeconvNets / UNets / Hourglass [31,101]密集连接[93]。

**保持高分辨率表示:** 高分辨率表示是在整个过程维持高分辨率，通常是通过将多分辨率（从高分辨率到低分辨率）并行卷积与并行卷积中的重复信息交换连接而形成的网络。
代表作品包括GridNet [30]，卷积神经结构[86]，相互连接的CNN [132]，以及最近开发的高分辨率网络（HRNet）[91]，这是我们感兴趣的。

两个早期的工作，卷积神经结构[86]和相互关联的CNN [132]，缺乏对何时启动低分辨率并行流以及在什么时候如何跨并行流交换信息的仔细设计，并且不使用批量标准化和剩余连接 因此没有表现出令人满意的表现。

GridNet [30]类似于多个U-Nets的组合，包括两个对称信息交换阶段：第一阶段仅将信息从高分辨率传递到低分辨率，第二阶段仅将信息从低分辨率传递到高分辨率 解析度。 这限制了其分割质量。

![2](/img/2019-07-01-2.png)

## 3. 学习高分辨率表示

为方便起见，我们将之前的高分辨率网络命名为HRNetV1。HRNetV1网络[91]通过并行连接高到低分辨率的卷积来保持高分辨率表示.其中，在并行卷积上存在重复的多尺度融合。

**架构：**该架构如图1所示。有四个阶段，第二，第三和第四阶段是通过重复模块化的多分辨率块形成的。 多分辨率块由多分辨率组卷积和多分辨率卷积组成，如图2（a）和（b）所示。 多分辨率组卷积是组卷积的简单扩展，其将输入通道划分为若干通道子集，并且分别在不同空间分辨率上对每个子集执行常规卷积。

多分辨率卷积如图2（b）所示。它类似于常规卷积的多分支全连接方式，如图2（c）所示。常规卷积可以分为多个小卷积，如[122]中所述。 输入通道分为几个子集，输出通道也分为几个子集。输入和输出子集以完全连接的方式连接，每个连接都是常规卷积。输出信道的每个子集是输入信道的每个子集上的卷积的输出的总和。

差异在于两个方面：（i）在多分辨率卷积中，每个信道子集的分辨率都不同。 （ii）输入通道和输出通道之间的连接需要进行处理。通过使用几个步长为2的3*3个卷积[91]实现分辨率降低。分辨率增加则简单的通过双线性[91]（最近邻）上采样来实现。

**修改：**在原始方法HRNetV1中，仅输出来自[91]中的高分辨率卷积的表示（特征图），如图3（a）所示。这意味着仅利用来自高分辨率卷积的输出通道的子集，并且丢失来自低分辨率卷积的其他子集。

![3](/img/2019-07-01-3.png)

我们通过利用从低分辨率卷积输出的其他通道子集进行简单而有效的修改。 好处是充分探索了多分辨率卷积的能力。此修改仅增加了一个小参数和计算开销。

我们通过双线性上采样将低分辨率表示重新缩放到高分辨率，并连接表示的子集，如图3（b）所示，从而产生高分辨率表示，我们采用它来估计分割图/面部标志热图。在应用于目标检测时，我们通过下采样高分辨率表示来构建多级表示，其中平均池化到多个级别，如图3（c）所示。 我们将这两个修改分别命名为`HRNetV2`和`HRNetV2p`，并在第4.4节中根据实验进行比较。

**实例化：**我们使用与HRNetV1 [91] 2类似的方式实例化网络。 网络从一个由两个步长3x3卷积组成的主干开始，将分辨率降低到1/4。 第一阶段包含4个残差单元，其中每个单元由宽度为64的瓶颈形成，然后是一个3x3卷积，将特征图的宽度减少到C.第2，第3，第4阶段分别包含1,4,3 个多分辨率块。 四种分辨率的卷积的宽度（通道数）分别为C，2C，4C和8C。 多分辨率组卷积中的每个分支包含4个残余单元，每个单元在每个分辨率中包含两个3x3个卷积。

在语义分割和面部标志检测的应用中，我们通过1x1的卷积从所有四种分辨率将输出表示混合（图3（b）），并产生15C维表示。然后，我们将每个位置的混合表示传递给具有softmax / MSE损失的线性分类器/回归器，以预测分割图/面部标志热图。对于语义分割，通过用于训练和测试的双线性上采样，将分割图上采样（4次）到输入大小。在应用于对象检测时，我们将高分辨率表示的维度通过1*1卷积减小到256，类似于FPN [62]，在形成图3（c）中的特征金字塔之前。

## 4. 实验

### 4.1、语义分割

语义分割是向每个像素分配类标签的问题。 我们在两个场景解析数据集PASCAL Context [71]和Cityscapes [19]以及人类解析数据集LIP [34]上对结果进行分析。采用类别交叉联合（mIoU）的平均值作为评估指标。

**城市场景：**Cityscapes数据集[19]包含5，000个高质量像素级精细注释的场景图像。这些精细注释的图像分为训练集，验证集和测试集。每一个集合分别包含2875、500、1525张图片。一共有30个类别，其中19个类别用于评估。除了mIoU之外，我们还在测试集上报告了其他三个分数：IoU类别（cat.），iIoU类（cla.）和iIoU类别（cat.）。

我们遵循相同的训练设置[126,127]。 通过随机裁剪（从1024x2048到512x1024）增加数据，并在在[0.5，2]的范围内随机缩放，同时随机水平翻转。我们使用SGD优化器，基本学习率为0.01，动量为0.9，权重衰减为0.0005。动量为0.9的多聚学习速率策略用于降低学习速率。 所有模型均在4个GPU和syncBN上均经过120K次迭代培训，batch size设置为12。

表1提供了在Cityscapes验证集上就参数和计算复杂性以及mIoU类方面而言的几种代表性方法的比较。 （i）HRNetV2-W40（40表示高分辨率卷积的宽度），与DeepLabv3 +具有相似的模型尺寸，并且计算复杂度低得多，性能更好：比UNet++增加4.7点，比DeepLabv3增加1.7点 与PSPNet，DeepLabv3+相比，大约增长0.5分。（ii）HRNetV2-W48，具有与PSPNet相似的模型尺寸和更低的计算复杂度，实现了显着的改进：比UNet ++增加5.6点，比DeepLabv3增加2.6点，比PSPNet增加约1.4点，DeepLabv3 +。 在以下比较中，我们采用在ImageNet 3上预训练的HRNetV2-W48，其模型尺寸与大多数基于Dilated-ResNet-101的方法相似。

表2提供了我们的方法与Cityscapes测试集上最先进方法的比较。 所有结果都有六个尺度和翻转。评估两种不使用粗略数据的情况：一种是关于在训练集上学习的模型，另一种是关于在训练+验证集上学习的模型。在这两种情况下，HRNetV2-W48达到最佳性能，并且比先前的最新技术优于1分。

![5](/img/2019-07-01-5.png)

**PASCAL**： PASCAL上下文数据集[71]包括4998个用于训练的场景图像和5105个用于测试的图像，具有59个语义标签和1个背景标签。

数据增强和学习率策略与Cityscapes相同。 根据广泛使用的训练策略[117,23]，我们将图像大小调整为480x480，并将初始学习率设置为0.004，并将权重衰减设置为0.0001。 批量大小为16，迭代次数为60K。

我们遵循标准测试程序[117,23]。 图像大小调整为480x480，然后输入我们的网络。 然后将得到的480x480标签贴图调整为原始图像尺寸。 我们使用六个尺度和翻转来评估我们的方法和其他方法的性能。

表3提供了我们的方法与最先进方法的比较。 评估方案有两种：在59个类的mIoU和60个类上的mIoU（59个类+背景）。 在这两种情况下，HRNetV2-W48都优于以前的技术水平。

![6](/img/2019-07-01-6.png)

