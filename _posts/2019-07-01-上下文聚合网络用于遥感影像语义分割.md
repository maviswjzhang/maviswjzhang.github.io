---
layout:     post
title:      用于航空影像语义分割的上下文聚合网络
subtitle:   Context Aggregation Network for Semantic Labeling in Aerial Images
date:       2019-07-01
author:     Nick
header-img: img/博客背景.jpg
catalog: true
tags:
    - 遥感
	- 语义分割
	- 论文阅读
---

## 摘要：

高分辨率航拍图像的语义标注是遥感图像分析的基本和必要任务。它广泛用于土地利用调查，变化检测和环境保护。最近的研究揭示了卷积神经网络（CNNs）在这项任务中的优越性。然而，多尺度物体识别和物体的精确定位是基于CNN的高分辨率航拍图像中语义标注方法的两个主要难题。为了解决这些问题，我们设计了一个用于聚合多个尺度的上下文信息Context Fuse模块，它由具有不同大小卷积核的并行卷积层和一个全局池化分支组成。同时，我们提出了一个Attention Mix模块，它利用通道方式的注意机制来组合多尺度特征，以获得更高的定位精度。最后，我们进一步采用残差卷积模块来优化所有尺度的特征。基于这些模块，我们构建了一个新的用于航空图像中的语义标记的端到端网络。我们在ISPRS Vaihingen和Potsdam数据集上评估所提出的网络。实验结果表明，在两个数据集上，当仅使用原始图像数据时，我们的网络优于其他竞争对手。

## 1. 简介

对于航拍图像，语义标记意味着为每个像素分配类别标签，这在计算机视觉领域中也称为的语义分割。在本质上，它是一个需要对航拍图像中的每个像素进行分类的多类别的分类问题[1-3]。这种特性使其比包括建筑物提取和道路提取等在类的二元分类问题更复杂。此外，与计算机视觉领域中广泛使用的数码照片不同，航拍图像由物体呈现大规模变化的复杂场景构成。航拍影像由多种物体组成，如建筑物，植被，树木等。而如建筑物大小不一，汽车很小，而树木则与植被交织在一起等问题使得语义标签任务更加困难。但是虽然存在困难，航空图像解译仍是的必要的任务，并且成为后续包括土地利用分析，环境保护，城市变化检测，城市规划等应用的基础[4-8]。

对于这项任务，研究人员提出了许多方法来完成它。这些方法可以分为传统方法和卷积神经网络（CNNs）两类方法。传统方法主要由特征提取和分类算法两个独立的部分组成。从航拍图像中的小补丁中提取某些类型的特征，然后将其发送到分类器以确定其类别。通常手动构建包括尺度不变特征变换（SIFT）[9]，定向梯度直方图（HOG）[10]和加速分段测试特征（FAST）[11]在内的特征。这些手工制作的特征虽然在特定条件下具有各自的特点，但都不能很好地处理一般情况。这导致研究人员需要仔细选择适合其特定情况的特征，否则他们必须设计自定义特征。传统方法使用的分类器主要是常规的机器学习算法，如广泛使用的K-means [12]，支持向量机[13]和随机树[14]等。然而，高分辨率航拍图像，通常具有复杂的场景。在城市区域这种场景更为复杂。同时，不同类别的物体呈现相似的外观。因此，这些传统方法无法为此任务获得满意的结果。

近年来，CNN已经在图像处理领域中表现出显着的性能。它可以从海量图像数据中自动构建特征，同时实现特征提取和分类。它在图像分类任务中表现出色[15]。截止目前，已经提出了例如VGG [16]，ResNet [17]和DenseNet [18]在内的许多用于图像分类的经典网络。

由于具有强大的识别能力和特征学习特性，CNN已被引入语义标注领域。许多基于图像补丁分类的CNN模型已经被设计用于此任务[19-23]。一般方法是通过滑动窗口从原始大图像中裁剪小块，然后用CNN对该块进行分类。由于具有优越的特征表达能力，该方法与传统方法相比表现更好。但是常规的补丁分区导致了结构信息的丢失。此外，由于滑动窗口方法所需的巨大迭代步骤，它需要大的计算成本[24]。作为改进，研究人员尝试使用了如超像素分割方法的结构分割算法。它生成了具有不规则形状的贴片并保留更多结构信息[25,26]。然后他们使用CNN模型来提取特征并对补丁进行分类。然而，该方法仍然使用与CNN模型分离的分割算法，因此承担了过早决策的风险。

为了克服这个困难，完全卷积网络（FCN）[27]被提了出来。 FCN删除VGG中全连接层并直接输出概率图。然后，它将概率图上采样到原始输入图像的相同大小。在某种意义上，FCN丢弃了分割部分，并生成语义标记作为像素级分类的自然结果。因此，与基于补丁的分类方法相比，FCN可以处理不规则的边界并获得更一致的结果。

虽然FCN比其他模型实现了更好的性能[20,21]，但仍存在两个局限性。首先，由于连续的下采样操作，特征图尺寸大大减小，因此最终特征图的空间分辨率大大降低。这意味着信息的大量丢失，直接导致了最后的语义标记结果丢失了大量细节，并且看起来模糊不清。其次，它直接使用骨干网络提取的特征，使得特征并没有被有效利用。这导致FCN在捕获多尺度特征和识别复杂场景方面很弱。 因此，对于具有多个尺度的对象，它无法很好地识别它们。在航拍图像中，由于场景的大规模变化性、复杂性以及对象的精细结构等原因，这个问题更加严重。

为了解决第一个问题，研究人员要么生成具有更高分辨率的特征图，要么更有效地利用浅层特征。例如，DeconvNet [28]使用连续的上池化和反卷积层来逐步恢复特征图解析。事实上，它采用的是编码器-解码器架构。 SegNet [29]在编码器部分中记录池化索引，然后利用池化索引信息在解码器中执行非线性上采样并获得更准确的位置信息。这消除了以学习方式进行上采样的需要。U-Net [30]提出了类似的编码器 - 解码器模型，并引入了低级特征，以改善解码器阶段的最终结果。 FRRN [31]设计了一个双流网络。一个流以完整图像分辨率传送信息以保持精确的边界。另一个流经过连续池化操作以获得用于识别的强大功能。 RefineNet [32]设计了一个多路径细化网络。它利用下采样过程中的信息来获取具有远程残差连接的高分辨率预测。

对于另一个问题，研究人员试图更广泛地利用CNN提取的特征。 PSPNet [33]通过空间金字塔池化获取的基于不同区域的上下文聚合来利用全局上下文信息。 DeepLab [34,35]使用并行空洞卷积运算来聚合多尺度特征，并在多个尺度上稳健地分割对象。GCN [36]验证了大卷积核的有效性，并应用全局卷积运算来捕获上下文信息。 EncNet [37]引入了上下文编码模块，该模块捕获场景的语义上下文并有选择地突出显示依赖于类的特征映射以捕获上下文信息。

在本文中，我们介绍了一种新的端到端网络，用于航空图像中的语义标记。它可以有效地处理上述问题。它是一种类似编码器-解码器的架构，具有高效的上下文信息聚合和基于注意力的多级特征融合。具体来说，我们设计了一个上下文融合模块（CFM），它由具有不同大小内核和全局池化分支的并行卷积层组成。前者用于聚合具有多个感受域的上下文信息。后者用于引入已在近期工作中证明有效的全局信息[33,35]。我们还提出了一个注意混合模块（AMM），它利用通道智能注意机制来组合多级特征，并有选择地强调更具辨别力的功能。我们进一步采用残差卷积模块（RCM）来优化所有特征级别的特征。基于这些模块，我们构建了一个新的端到端网络，用于航空图像中的语义标记。我们在ISPRS Vaihingen和Potsdam数据集上评估所提出的网络。实验结果表明，我们的网络优于其他最先进的基于CNN的模型和仅使用原始图像数据的顶级方法。

我们的贡献有如下几点：

1）我们设计了一个Context Fuse模块来广泛地利用上下文信息。 它由具有不同大小卷积核的并行卷积层以及用于引入全局信息的全局池化分支组成。其中并行卷积层用于聚合具有多个感知域的上下文信息。

2）我们提出了一个注意混合模块，它利用通道方式的注意机制来组合多级特征，并有选择地强调更具辨别力的特征以进行识别。我们进一步采用残差卷积模块来优化所有特征级别的特征

3）基于这些模型，我们构建了一个新的端到端网络，用于航空图像中的语义标记。 我们在ISPRS Vaihingen和Potsdam数据集上评估所提出的网络。实验结果表明，我们的网络在没有使用数字表面模型信息而仅使用原始图像数据的情况下，优于其他最先进的基于CNN的模型和基准测试方法。

4）我们在https://github.com/Spritea/Context-Aggregation-Network上公开提供基于PyTorch的模型实现。

## 2. 方法描述

我们将在本节中详细介绍我们所提出的端到端网络的各个部分。首先详细说明所提出的上下文融合模块（CFM），注意混合模块（AMM）和残余卷积模块（RCM）。 然后，我们介绍了基于这些模型的用于航空图像中的语义标记的上下文聚合网络（CAN）。图1显示了CAN的整体框架。

![1](/img/2019-07-01-7.png)

### 2.1. Context Fuse Module

在语义标注的任务中，上下文信息是至关重要的。由于航空图像中复杂场景的大规模变化，上下文信息对于准确的语义标记更是必不可少的。 例如，PSPNet [33]应用空间金字塔池化来融合不同级别的特征。Ref.[38]使用级联空洞卷积层来扩大感受野，而无需额外的参数。DeepLabv3 [35]采用具有全局池化层的并行空洞卷积层来组合多尺度特征。 然而，空间金字塔池化[33]本质上是一种下采样操作，会丢失一些细节信息。空洞卷积[34]将导致网格效应[39]并生成带有棋盘图案的特征图。为了克服这个问题，我们提出了CFM。它可以有效地聚合具有不同感受域和全局上下文信息的特征。

![2](/img/2019-07-01-8.png)

我们的CFM块主要由两部分组成，如图2所示.A部分是一个并行卷积块，包含4个具有不同大小卷积内核的分支。B部分是一个全局池分支，用于引入全局上下文信息。

**并行卷积块**。并行卷积块由4个具有不同大小的的卷积分支组成，即{3,7,11,15}。 每个分支具有两个卷积层以提取由骨干网络生成的特征。我们还使用批量标准化[40]来减少内部协变量偏移并使用ReLU [41]作为激活函数。 与DeepLabv3中的ASPP不同[35]，我们使用常规卷积层（实心内核），而不是空洞卷积层（零填充内核）。 通过这种方式，我们可以防止特征图具有网格效果并获得更一致的结果。对于每个卷积分支提取的特征，我们将它们连接在一起以融合具有不同感知域的特征。

至于卷积核大小，最大的是15 * 15卷积核。 原因是ResNet [17]网的最后一个卷积层输出的是输入图像1/32大小的特征映射。 这里的输入图像大小是512 * 512，最后一个特征映射大小是16 * 16.然后这个15 * 15卷积分支便可以成为全局卷积[36]。这增加了网络识别复杂对象的能力。我们可以根据输入图像大小调整内核大小。此外，由于最后的特征图很小，这些大的核卷积层只会增加很小的额外计算成本。

**全局池化分支**。 事实证明，全局池化在最近的工作中是有效的[33,35,42]。 PSPNet [33]利用金字塔池模块中的全局池化来引入全局上下文信息。 DeepLabv3 [35]采用全局池化作为对复杂的空间金字塔池化的补充。 ParseNet [42]采用额外的全局上下文来解决局部混淆和平滑分割。

我们还采用全球平均池化来引入全局信息。在此分支中，特征图通过全局平均池层来捕获全局上下文，然后通过1x1卷积层执行通道缩减。然后对最终特征图进行上采样并与并行卷积块输出连接。

总而言之，并行卷积块通过具有不同大小卷积核的卷积层聚合多尺度信息，全局池化分支捕获全局上下文信息。因此，CFM块生成包含多尺度信息和全局上下文信息的特征。 由于大规模变化和复杂场景，这些特征对于航拍图像中的语义标记是必需的。

### 2.2. Attention Mix Module

对于语义标记网络，深层特征包含具有低空间分辨率的高级语义信息，而浅层特征包含具有高空间分辨率的低级结构信息。为了获得更准确的位置信息，许多研究将浅层特征与深层特征相结合。FCN [27]使用跳过连接将深层特征添加到深层特征。U-Net [30]逐步将浅层特征与深层特征连接起来进行特征融合。 DeepLabv3 + [43]在一个浅层特征上进行通道缩减，并将其与一个深层连接。

![3](/img/2019-07-01-9.png)

但是，这些研究要么直接对它们求和[27]，要么将它们连接在一起[30,43]，而不考虑通道之间的差异。受SENet [44]的启发，我们认为卷积特征通道之间的相互依赖很重要。因此，如图3所示，我们设计AMM模块以结合低级特征和高级特征。

具体而言，AMM首先将低级特征和高级特征连接在一起，然后使用3×3卷积层来执行信道缩减。之后，使用全局平均合并将要素图缩小为1x1，并成为向量。 矢量与其自身相乘作为通道的权重。通过这种方式，我们强调了具有更强判别力特征通道，并抑制较少的歧视性特征通道。最后，我们直接向它添加低级特征以执行显式融合，这使得该模块成为类似残差的结构，并与残余块具有相似的好处[17]。因此，我们更加自适应地融合多级功能，并获得具有更高识别能力的特征。

### 2.3. Residual Convolutional Module

![4](/img/2019-07-01-10.png)

### 2.4. Context Aggregation Network

基于提出的模块，我们构建了用于航空图像中语义标记的CAN模型。如图1所示，这是一种类似于编码器-解码器[30] 的体系结构。ResNet [17]因其强大的特征提取能力而被用作骨干网络。对于ResNet中每个阶段的输出，它们通过RCM块进行特征适应和细化。 然后，最后一层的特征图（包含丰富的高级语义信息）被输入CFM块以捕获和聚合多尺度特征。之后，通过AMM块组合在骨干网的多个阶段中生成的不同级别的特征映射。这些组合的特征由RCM块改进，然后网络输出最终的语义标记结果。

## 3. 实验结果

### 3.1 实验设置

我们对每个数据集进行了两种评估，即本地评估和基准评估，以进行综合比较，与[48]类似。 本地评估将提出的模型与其他最先进的深度学习模型进行比较。 在线评估是将提出的模型与领先的基准模型进行比较。

对于ISPRS Vaihingen数据集中的本地评估，我们随机选择23个图像作为训练集，其他10个图像作为测试集。对于相应的基准评估，我们遵循基准测试中的数据划分方式，即16个训练图像和17个测试图像，以便与其他基准模型进行比较。 对于ISPRS Potsdam数据集中的局部评估，我们随机选择18个图像作为训练集，因为图像尺寸比Vaihingen数据集（6000×6000 v.s. 2500×2000）大得多，其余作为测试集。对于相应的基准评估，我们还遵循基准测试中的数据分区方式，选择24个图像作为训练集，14个作为测试集。 需要注意的是，对于两个数据集中的局部评估，我们使用全标签图像作为地面实况图像来设置更高的标准并获得更准确的结果。 对于基准评估，我们使用侵蚀的标签图像与这些基准方法保持一致。

由于GPU内存限制，原始大型训练图像被裁剪，重叠100像素到512x512像素的补丁。 测试图像被裁剪为相同尺寸的贴片而没有重叠。 由于数据集对于深度学习方法来说很小，我们每90度应用水平翻转，垂直翻转和旋转以增强训练集。 我们还调整了原始大图像的大小，其系数为{0.5,0.75,1.25,1.5}并裁剪它们以扩大训练集。

######  **训练细节**

* 1）采用ResNet-50在ImageNet上的预训练权重。
* 2）batch Size = 12. network is trained on two Titan V GPUs, with 12 GB memory per GPU. 
* 3）初始化学习率为0.1，此后每50轮学习率缩小为原来的1/10。
* 4）采用F1得分、OA、IoU作为评价指标。

### 3.2 实验结果

![vaihingen val](/img/2019-07-01-11.png)

![potsdam val](/img/2019-07-01-12.png)

![vaihingen test](/img/2019-07-01-13.png)

![potsdam test](/img/2019-07-01-14.png)

