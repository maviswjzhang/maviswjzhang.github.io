---
layout:     post
title:      LEDNET：用于语义分割的轻量级编码器解码器网络
subtitle:   LEDNET: A LIGHTWEIGHT ENCODER-DECODER NETWORK FOR REAL-TIME SEMANTIC SEGMENTATION
date:       2019-08-18
author:     Nick
header-img: img/博客背景.jpg
catalog: true
tags:
    - 语义分割
    - 论文阅读
---

**写在前面**

[论文地址](https://arxiv.org/abs/1905.02423)

[github代码](https://github.com/xiaoyufenfei/LEDNet#Resuming-training-if-decoder-part-broken)

## 1.  摘要

算力负担限制了移动设备中CNN在密集估计任务中的使用。在本文中，我们提出了一个轻量级网络来解决这个问题，即 LEDNet，它采用**非对称（asymmetric）编码器 - 解码器架构**来进行实时语义分割。更具体地说，编码器采用 ResNet 作为骨干网络，其中有两个新操作：**channel split and shuffle**，被应用在每个残余块中，以大大降低计算成本，同时保持更高的分割精度。 另一方面，在解码器中采用**注意力金字塔网络（APN，attention pyramid network）**以进一步减轻整个网络的复杂性。我们的模型参数不到1M，并且能够在**单个GTX 1080Ti GPU中以超过71 FPS的速度**运行。全面的实验表明，我们的方法在 CityScapes 数据集的速度和准确性权衡方面取得了SOTA。

## 2. 简介

简单来说，本文在模型结构上共有三个改进和贡献：

* **asymmetric encoder- decoder（非对称架构）：**导致网络参数大大减少，加快了推理过程；
* **split-shuffle-non-bottleneck(SS-nbt)：**残差网络中的 Channel split and shuffle有强大的特征表示。此外， Channel shuffle 是可谓的，可以嵌入网络结构中进行端到端训练；
* **attention pyramid net- work (APN)：**decoder 端采用特征金字塔的注意力机制，进一步降低了整个网络的复杂性；

## 3. 模型结构

![img](/img/2019-08-18-1.png)

![2019-08-18-2](/img/2019-08-18-2.png)

### 3.1 Residual Module with Split and Shuffle Operations
**描述：**在每个SS-nbt的开始，输入被分成两个低维分支，每个分支有输入通道的一半。为了避免点卷积，使用一组特殊的一维卷积来执行变换，两个分支的卷积输出使用串联来合并，这样通道的数量保持不变。为了便于训练，通过恒等映射分支进行残差连接。最后使用通道洗牌操作来实现两个分支之间的信息通信。

**优点：**很明显，我们的残差模块不仅效率高，而且精度高。首先，每个SS-nbt的高效率使得我们可以使用更多的特征通道。其次，在每个SS- nbt单元中，将合并后的特征通道随机打乱，然后引入到下一个单元中。这可以看作是一种特征重用，在一定程度上增加了网络容量，但没有显著增加复杂性。

![img](/img/2019-08-18-6.png)

### 3.2  下采样模块

**描述：**叠加两个并行输出：一个步幅为2的3 x 3卷积，一个最大池化

**优点：**下采样使更深层次的网络能够收集上下文，同时有助于减少计算量。

### 3.3 Attention Pyramid Network (APN)

**描述：**APN采用了一个金字塔注意力模块，它集成了来自三个不同金字塔尺度的特征。

* 首先利用步幅为2的3 × 3, 5 × 5, and 7 × 7卷积来形成特征金字塔；
* 然后金字塔结构逐步上采样并融合不同尺度的信息；
* 对编码器的输出进行1 x 1卷积，然后卷积特征图被金字塔注意力产生的特征加权；
* 为了进一步提高性能，引入了一个全局平均池化分支来获取全局上下文，并将结果与加权过的特征图相加；
* 最后，利用上采样单元匹配输入分辨率。

**优点：**

* 由于高级特征图的分辨率较小，使用较大的卷积核大小不会带来太多的计算负担；
* 得益于金字塔结构，APN可以捕获多尺度的上下文信息；
* 与DeepLab[5]和PSPNet[8]不同，DeepLab[5]和PSPNet[8]是多尺度特征图的堆叠，我们的上下文信息与原始卷积特征进行了逐像素相乘，没有引入太多的计算预算。

## 4. 实验部分

### 4.1 执行设置

**数据：**训练（2975），验证（500），测试（1525）+ **粗糙标签（20K）**

**评价指标：**IoU，running time，FPS，model size

**机器：**单个1080Ti

**batch size：**5

**学习率：**5x10-4，poly learning rate policy with power 0.9

**优化器：**SGD，0.9的momentum，10-4的decay

### 4.2 实验结果

![2019-08-18-3](/img/2019-08-18-3.png)

![2019-08-18-4](/img/2019-08-18-4.png)

![2019-08-18-5](/img/2019-08-18-5.png)
