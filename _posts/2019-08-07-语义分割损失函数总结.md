---

layout:     post
title:      语义分割损失函数总结（持续更新）
subtitle:   语义分割
date:       2019-08-06
author:     Nick
header-img: img/博客背景.jpg
catalog: true
tags:
    - 语义分割， Keras
---

***总结来说，交叉熵平等对待每个像素，加权交叉熵更关注少样本类别，focal loss更加关注难分样本，dice loss和iou loss更加关注TP,平等对待FN和FP，tversky loss除过TP外，更加倾向于关注FN***

## 1. 交叉熵（Cross Entorpy）

图像分割中最常用的损失函数是逐像素交叉熵损失。该损失函数分别检查每个像素，将类预测(softmax or sigmoid)与目标向量（one hot）进行比较。

### 1.1 理论指导

**二分类：**二分类最终模型采用sigmoid激活函数，最后一层仅包含一个通道，在每个像素处，得到的类别概率为`p`或1`-p`.

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-1.png)

其中，y表示样本的label，正类为1，负类为0。p表示样本被预测为正类的概率。

**多分类：**多分类就是直接将二分类的两项扩展成多项，项数等于类别数.

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-2.png)

其中, `M`表示类别数量；`yc`是指示变量（0或1）,如果该类别和样本的类别相同就是1，否则是0；`pc`代表观测样本属于`c`类别的预测概率。

### 1.2 python 实现示例

```
# 二分类
import numpy as np

def binary_cross_entropy(prediction, label):
    loss = -(label * np.log(prediction) + (1 - label) * np.log(1-prediction))
    return loss

p = np.random.random((5,))
y = np.round(np.random.random((5,)))

total_loss = 0
for i in range(len(p)):
    total_loss += binary_cross_entropy(p[i], y[i])
    mean_loss = total_loss / len(p)

# p: [0.23646617 0.97911987 0.69052516 0.50211481 0.39236193]
# y: [1. 1. 0. 1. 1.]
# mean_loss: 0.8520853731070185
```

```python
# 多分类
import numpy as np
from keras.utils import to_categorical

def category_cross_entropy(prediction, label):
    pre
    loss = label * np.log(prediction)
    loss = -np.sum(loss)
    return loss

p = np.random.random((5,3))
y = np.random.randint(0, 3, (5, 1))
onehot_y = to_categorical(y, num_classes=3)

total_loss = 0
for i in range(len(p)):
    total_loss += category_cross_entropy(p[i], onehot_y[i])

# p
[[0.44738859 0.50149158 0.99165305]
 [0.66047294 0.55890212 0.49924387]
 [0.2049042  0.99509507 0.47469392]
 [0.84587505 0.37910283 0.42233321]
 [0.17514246 0.20649966 0.64933319]]
# y
[[1]
 [0]
 [2]
 [1]
 [1]]
# one hot y
[[0. 1. 0.]
 [1. 0. 0.]
 [0. 0. 1.]
 [0. 1. 0.]
 [0. 1. 0.]]
# mean_loss
1.3294440631054194
```

### 1.3 keras内部源码

#### 1.3.1 二分类

```python
def binary_crossentropy(y_true, y_pred):
    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)
    
def binary_crossentropy(target, output, from_logits=False):
    """Binary crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor with the same shape as `output`.
        output: A tensor.
        from_logits: Whether `output` is expected to be a logits tensor.
            By default, we consider that `output`
            encodes a probability distribution.

    # Returns
        A tensor.
    """
    # Note: tf.nn.sigmoid_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # transform back to logits
        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)
        output = tf.log(output / (1 - output))

    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,
                                                   logits=output)
```

binary_crossentropy存在一个默认为False的参数from_logits(深度学习源码中经常出现的`logits`就是定义的神经网络的未通过激活函数的原始输出)。通常我们只需要关注`if not from_logits:`这个分支的代码，其就是对做过sigmoid的输出进行还原，就是把原始输出还原出来，具体步骤如下：

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-3.png)

还原出来之后，就可以调用TF的原生接口计算二元交叉熵。下面与上面一样计算具体的例子：

```python
import keras.losses as loss
import numpy as np
import tensorflow as tf

p = np.array([0.23646617, 0.97911987, 0.69052516, 0.50211481, 0.39236193])
p = tf.convert_to_tensor(p.astype(np.float32))
y = np.array([1., 1., 0., 1., 1.])
y = tf.convert_to_tensor(y.astype(np.float32))

with tf.Session() as sess:
     print(sess.run(loss.binary_crossentropy(y, p)))

# loss: 0.8520853
```

#### 1.3.2 多分类

```python
def categorical_crossentropy(y_true, y_pred):
    return K.categorical_crossentropy(y_true, y_pred)
    
def categorical_crossentropy(target, output, from_logits=False, axis=-1):
    """Categorical crossentropy between an output tensor and a target tensor.

    # Arguments
        target: A tensor of the same shape as `output`.
        output: A tensor resulting from a softmax (unless `from_logits` is True, in which case `output` is expected to be the logits).
        from_logits: Boolean, whether `output` is the result of a softmax, or is a tensor of logits.
        axis: Int specifying the channels axis. `axis=-1` orresponds to data format `channels_last`, and `axis=1` corresponds to data format `channels_first`.

    # Returns
        Output tensor.

    # Raises
        ValueError: if `axis` is neither -1 nor one of
            the axes of `output`.
    """
    output_dimensions = list(range(len(output.get_shape())))
    if axis != -1 and axis not in output_dimensions:
        raise ValueError(
            '{}{}{}'.format(
                'Unexpected channels axis {}. '.format(axis),
                'Expected to be -1 or one of the axes of `output`, ',
                'which has {} dimensions.'.format(len(output.get_shape()))))
    # Note: tf.nn.softmax_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # scale preds so that the class probas of each sample sum to 1
        output /= tf.reduce_sum(output, axis, True)
        # manual computation of crossentropy
        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)
        return - tf.reduce_sum(target * tf.log(output), axis)
    else:
        return tf.nn.softmax_cross_entropy_with_logits(labels=target,
                                                       logits=output)  
```

categorical_crossentropy中，由于难以对softmax的输出进行还原，故使用了自定义代码计算，而没有调用TF接口，下面是一个计算实例，其输出结果与自定义的也一样：

```python
import keras.losses as loss
import numpy as np
import tensorflow as tf
from keras.utils import to_categorical

p = np.array([[0.44738859, 0.50149158, 0.99165305],
              [0.66047294, 0.55890212, 0.49924387],
              [0.2049042,  0.99509507, 0.47469392],
              [0.84587505, 0.37910283, 0.42233321],
              [0.17514246, 0.20649966, 0.64933319]])
p = tf.convert_to_tensor(p.astype(np.float32))
y = np.array([[1], [0], [2], [1], [1]])
y = to_categorical(y, num_classes=3)
y = tf.convert_to_tensor(y.astype(np.float32))

with tf.Session() as sess:
     print(sess.run(tf.reduce_mean(loss.categorical_crossentropy(y, p))))
        
# loss: 1.3294442
```

## 2. 加权交叉熵损失函数

在实践中，经常会遇到样本不均衡的问题，无论是二分类还是多分类，因此除过在数据层次的重采样，我们还可以设置加权交叉熵损失函数来解决样本不均衡的问题。

### 2.1 理论指导

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-4.png)

与之前相比，多了一个权重系数`a`。假设k类里面，第i类是少数类，为了加大少数类错分的成本，在交叉熵上给第i类乘以一个大于1的系数，这样如果分错第i类的话，交叉熵loss就会增加。

### 2.2 keras 实现

#### 2.2.1 二分类

```python
class WeightedBinaryCrossEntropy(object):
    def __init__(self, pos_ratio):
        neg_ratio = 1. - pos_ratio
        self.pos_ratio = tf.constant(pos_ratio, tf.float32)
        self.weights = tf.constant(neg_ratio / pos_ratio, tf.float32)
        self.__name__ = "weighted_binary_crossentropy({0})".format(pos_ratio)

    def __call__(self, y_true, y_pred):
        return self.weighted_binary_crossentropy(y_true, y_pred)

    def weighted_binary_crossentropy(self, y_true, y_pred):
        # Transform to logits
        epsilon = tf.convert_to_tensor(K.common._EPSILON, y_pred.dtype.base_dtype)
        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)
        y_pred = tf.log(y_pred / (1 - y_pred))

        cost = tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.weights)
        return K.mean(cost * self.pos_ratio, axis=-1)
```

#### 2.2.2 多分类

```python
class WeightedCategoricalCrossEntropy(object):
 
  def __init__(self, weights):
    nb_cl = len(weights)
    self.weights = np.ones((nb_cl, nb_cl))
    for class_idx, class_weight in weights.items():
      self.weights[0][class_idx] = class_weight
      self.weights[class_idx][0] = class_weight
    self.__name__ = 'w_categorical_crossentropy'
 
  def __call__(self, y_true, y_pred):
    return self.w_categorical_crossentropy(y_true, y_pred)
 
  def w_categorical_crossentropy(self, y_true, y_pred):
    nb_cl = len(self.weights)
    final_mask = K.zeros_like(y_pred[..., 0])
    y_pred_max = K.max(y_pred, axis=-1)
    y_pred_max = K.expand_dims(y_pred_max, axis=-1)
    y_pred_max_mat = K.equal(y_pred, y_pred_max)
    for c_p, c_t in itertools.product(range(nb_cl), range(nb_cl)):
        w = K.cast(self.weights[c_t, c_p], K.floatx())
        y_p = K.cast(y_pred_max_mat[..., c_p], K.floatx())
        y_t = K.cast(y_pred_max_mat[..., c_t], K.floatx())
        final_mask += w * y_p * y_t
    return K.categorical_crossentropy(y_pred, y_true) * final_mask
```

## 3. Focal Loss

### 3.1 理论指导

Focal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘。其是基于交叉熵损失函数的一种改进，下面一步一步来看。

**首先回顾上面二分类交叉熵损失函数：**

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-5.png)

y'是sigmoid函数的输出，介于0-1之间。所以，直观来说，对于正样本来说，输出概率越大损失值越小；对于负样本而言，输出概率越小则损失函数越小。所以，对于大量的简单样本会淹没掉困难样本在网络更新中起到主要作用，使得网络无法优化至最优。

**简单起见，我们定义符号pt：**

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-6.png)

**重新表示：**

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-7.png)

**二分类加权交叉熵损失函数：**

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-8.png)

![2019-08-07-9](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-9.png)

这种方式相当于加大少样本类别的权重。虽然权重a能够平衡**正负样本**的重要性，但是不能区分**易分和难分**的样本，其中，简单易分的样本(pt>0.5)，而难分的样本(pt<0.5)。

**Focal Loss的改进如下：**

Focal Loss 提出将损失函数降低易分样本的权重，并关注于对难分样本(pt<0.5)的训练：

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-10.png)

也就是说，**难分样本的(1-pt > 0.5), 易分样本的(1-pt < 0.5)**,最终就能起到加大难分样本权重的作用。此外，调制因子r在实验中发现r=2效果最好，并且其能够控制易分样本对于损失函数的贡献。

**实际使用：**

论文中实际采用的是加权的focal loss：

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-11.png)

### 3.2 keras实现

```python
def focal_loss(gamma=2., alpha=0.25):
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        loss = -K.sum(alpha * K.pow(1 - pt_1, gamma) * K.log(pt_1))
               -K.sum((1-alpha) * K.pow(pt_0, gamma) * K.log(1 - pt_0))
        return loss
    return focal_loss_fixed
```

## 4. Dice loss

### 4.1 理论指导

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-12.png)

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-13.png)

![2019-08-07-14](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-14.png)

![2019-08-07-15](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-15.png)

**dice loss 比较适用于样本极度不均的情况，使用时还是建议使用dice loss和bce loss一同优化。**

### 4.2 keras实现

```python
def dice_loss(y_true, y_pred):
    intersection = tf.reduce_sum(tf.multiply(y_true, y_pred))
    union = tf.reduce_sum(tf.square(y_true)) + tf.reduce_sum(tf.square(y_pred))
    loss = 1. - 2 * intersection / (union + K.epsilon())
    return loss
```

## 5. IoU (Jaccard) Loss

### 5.1 理论指导

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-16.png)

**`IoU loss = 1-J(A, B)`**

### 5.2 keras 实现

```PYTHON
def jaccard_loss(y_true, y_pred):
    intersection = tf.reduce_sum(tf.multiply(y_true, y_pred))
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    loss = 1. - intersection / (union + K.epsilon())
    return loss
```

## 6. Tversky loss

### 6.1 理论指导

Tversky loss是dice loss一种扩展形式，因为**dice loss会平等地权衡FP和FN（精度和召回）**。然而在医学数据中，病变体素的数量通常远低于非病变体素的数量。使用不平衡数据进行训练可能导致严重**偏向于高精度但低召回率**（灵敏度）的预测，这在医疗应用中是不期望的，因此Tversky loss希望在训练高度不平衡数据的网络时权衡FN（召回）而不是FP：

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-17.png)

其中α和β分别控制FP和FN的惩罚幅度，P、G分别代表预测和真值。

形象的来看，就是希望基于FN和FP区域不同的权重，让loss更加关注FN，而不是平等的关注FN和FP。

![img](C:\Users\CV\Documents\GitHub\niecongchong.github.io\img\2019-08-07-19.png)

### 6.2 keras实现

```PYTHON
def tversky_loss(y_true, y_pred):
    y_true_pos = K.flatten(y_true)
    y_pred_pos = K.flatten(y_pred)
    # TP
    true_pos = K.sum(y_true_pos * y_pred_pos)
    # FN
    false_neg = K.sum(y_true_pos * (1-y_pred_pos))
    # FP
    false_pos = K.sum((1-y_true_pos) * y_pred_pos)
    alpha = 0.7
    return 1 - (true_pos + K.epsilon())/(true_pos + alpha * false_neg + (1-alpha) * false_pos + K.epsilon())
```

## 7. 